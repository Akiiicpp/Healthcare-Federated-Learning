# Healthcare Federated Learning Platform - Complete System Flow
# ============================================================

## 1. OVERVIEW
This document explains the complete flow of the AI model, federated learning process, API integration, and all components in the Healthcare Federated Learning Platform.

## 2. SYSTEM ARCHITECTURE

### Core Components:
- **Coordinator**: Central server managing federated learning rounds
- **Hospital Clients**: Distributed nodes (simulated hospitals) performing local training
- **API Server**: Flask-based monitoring and data collection service
- **Dashboard**: Streamlit-based real-time visualization (planned)
- **Models**: PyTorch-based CNN for medical image classification

### Technology Stack:
- **Federated Learning**: Flower framework
- **Deep Learning**: PyTorch with custom ResNet-18
- **API**: Flask with CORS support
- **Data**: Synthetic medical images (real datasets supported)
- **Privacy**: Differential Privacy (Opacus)
- **Deployment**: Docker + docker-compose

## 3. DATA FLOW

### 3.1 Data Generation
```
SyntheticMedicalDataset (data_loader.py)
â”œâ”€â”€ Input: num_samples, image_size, pos_ratio, seed
â”œâ”€â”€ Process: Generate random noise + cancer signal patterns
â”œâ”€â”€ Output: PyTorch Dataset with (image, label) pairs
â””â”€â”€ Labels: 0=normal, 1=cancer
```

### 3.2 Data Partitioning
```
Hospital Data Distribution:
â”œâ”€â”€ Hospital 1: Urban (30% data, pos_ratio=0.25)
â”œâ”€â”€ Hospital 2: Rural (15% data, pos_ratio=0.20)
â”œâ”€â”€ Hospital 3: Children's (10% data, pos_ratio=0.15)
â”œâ”€â”€ Hospital 4: Cancer Center (25% data, pos_ratio=0.35)
â””â”€â”€ Hospital 5: General (20% data, pos_ratio=0.30)

Non-IID Simulation: Each hospital gets different data distribution
```

### 3.3 Data Loading
```
DataLoader Pipeline:
â”œâ”€â”€ Batch size: 32 (configurable)
â”œâ”€â”€ Validation split: 20%
â”œâ”€â”€ Shuffle: True for training
â”œâ”€â”€ Augmentation: None (synthetic data is pre-augmented)
â””â”€â”€ Device: MPS/CUDA/CPU
```

## 4. AI MODEL FLOW

### 4.1 Model Architecture
```
SmallResNet18 (models/cnn_model.py)
â”œâ”€â”€ Input: 1-channel grayscale images (224x224)
â”œâ”€â”€ Stem: Conv7x7 + BN + ReLU + MaxPool
â”œâ”€â”€ Layers: 4 residual blocks (64â†’128â†’256â†’512 channels)
â”œâ”€â”€ Head: AdaptiveAvgPool + Flatten + Linear(512â†’2)
â””â”€â”€ Output: Logits for binary classification
```

### 4.2 Model Initialization
```
create_model() â†’ SmallResNet18
â”œâ”€â”€ Parameters: ~11.2M (lightweight vs ResNet-50's 25M)
â”œâ”€â”€ Initialization: Kaiming normal for conv, constant for BN
â”œâ”€â”€ Device: Auto-detect MPS/CUDA/CPU
â””â”€â”€ Optimizer: Adam (lr=1e-3)
```

### 4.3 Training Loop (Local)
```
Single Epoch Flow (train_one_epoch):
â”œâ”€â”€ For each batch:
â”‚   â”œâ”€â”€ Zero gradients
â”‚   â”œâ”€â”€ Forward pass â†’ logits
â”‚   â”œâ”€â”€ CrossEntropy loss
â”‚   â”œâ”€â”€ Backward pass
â”‚   â”œâ”€â”€ Optimizer step
â”‚   â””â”€â”€ Accumulate loss
â””â”€â”€ Return: average loss per sample
```

### 4.4 Evaluation
```
evaluate() function:
â”œâ”€â”€ Model.eval() mode
â”œâ”€â”€ No gradients (torch.no_grad())
â”œâ”€â”€ For each batch:
â”‚   â”œâ”€â”€ Forward pass
â”‚   â”œâ”€â”€ Loss calculation
â”‚   â”œâ”€â”€ Predictions (argmax)
â”‚   â””â”€â”€ Probabilities (softmax)
â”œâ”€â”€ Metrics: loss, accuracy, AUC-ROC
â””â”€â”€ Return: Dict with all metrics
```

## 5. FEDERATED LEARNING FLOW

### 5.1 Initialization Phase
```
Coordinator Setup:
â”œâ”€â”€ Load global model
â”œâ”€â”€ Get initial parameters
â”œâ”€â”€ Configure Flower strategy (FedAvg)
â”œâ”€â”€ Set min_clients, rounds, local_epochs
â””â”€â”€ Start server on port 8080
```

### 5.2 Round-Based Training
```
For each round (1 to N):
â”œâ”€â”€ Coordinator broadcasts global model
â”œâ”€â”€ Hospitals receive model parameters
â”œâ”€â”€ Local Training Phase (per hospital):
â”‚   â”œâ”€â”€ Load local data
â”‚   â”œâ”€â”€ Train for local_epochs
â”‚   â”œâ”€â”€ Evaluate on local validation
â”‚   â””â”€â”€ Send updated parameters + num_samples
â”œâ”€â”€ Aggregation Phase:
â”‚   â”œâ”€â”€ Weighted average by dataset size
â”‚   â”œâ”€â”€ Update global model
â”‚   â””â”€â”€ Evaluate global performance
â””â”€â”€ API Updates: Send metrics to monitoring
```

### 5.3 Client Flow (Hospital)
```
HospitalClient (NumPyClient):
â”œâ”€â”€ fit(): Local training + parameter return
â”œâ”€â”€ evaluate(): Local validation + metrics
â”œâ”€â”€ get_parameters(): Model serialization
â””â”€â”€ set_parameters(): Model deserialization
```

### 5.4 Aggregation
```
FedAvg Implementation:
â”œâ”€â”€ Collect (parameters, num_samples) from all clients
â”œâ”€â”€ Weighted sum: sum(params * samples) / total_samples
â”œâ”€â”€ Update global model
â””â”€â”€ Handle parameter conversion (torch â†” numpy)
```

## 6. API FLOW

### 6.1 Flask Application Structure
```
create_app() â†’ Flask app
â”œâ”€â”€ In-memory state storage
â”œâ”€â”€ CORS enabled for frontend
â”œâ”€â”€ Routes: /api/* endpoints
â””â”€â”€ Internal routes: /api/internal/* for coordinator
```

### 6.2 Public Endpoints
```
GET /api/health
â””â”€â”€ Returns: {"status": "ok", "time": "ISO"}

GET /api/rounds/current
â””â”€â”€ Returns: {"round": current_round}

GET /api/metrics/latest
â””â”€â”€ Returns: Latest round metrics or 204 if none

GET /api/metrics/history?limit=N&offset=M
â””â”€â”€ Returns: Paginated metrics history

GET /api/clients
â””â”€â”€ Returns: List of registered clients
```

### 6.3 Internal Endpoints (Coordinator â†’ API)
```
POST /api/internal/round
â”œâ”€â”€ Payload: {"round": int}
â””â”€â”€ Updates current round number

POST /api/internal/metrics
â”œâ”€â”€ Payload: {"round": int, "loss": float, "accuracy": float, "timestamp": str}
â””â”€â”€ Adds metrics to history (bounded to 10K entries)

POST /api/internal/clients
â”œâ”€â”€ Payload: {"cid": str, "status": str}
â””â”€â”€ Updates or adds client information
```

### 6.4 Data Storage
```
In-Memory State (for demo):
â”œâ”€â”€ round: int (current round)
â”œâ”€â”€ metrics: List[Dict] (time-series data)
â””â”€â”€ clients: List[Dict] (client registry)

Production: Replace with database (PostgreSQL/Redis)
```

## 7. EXECUTION FLOWS

### 7.1 Local Training (Phase 1)
```
run_local_training.py:
â”œâ”€â”€ Create model + optimizer
â”œâ”€â”€ Generate synthetic data (800 samples)
â”œâ”€â”€ Train 2 epochs locally
â”œâ”€â”€ Evaluate final performance
â”œâ”€â”€ Save model to local_model.pth
â””â”€â”€ Expected: 100% accuracy on synthetic data
```

### 7.2 Federated Training (Phase 2)
```
run_federated.sh:
â”œâ”€â”€ Start coordinator server (background)
â”œâ”€â”€ Start N hospital clients (background)
â”œâ”€â”€ Wait for server completion
â”œâ”€â”€ Cleanup client processes
â””â”€â”€ Expected: Accuracy 33% â†’ 100% over rounds
```

### 7.3 Docker Deployment (Phase 4)
```
docker-compose.yml:
â”œâ”€â”€ coordinator: Flask/Flower server
â”œâ”€â”€ hospital-client: Multiple instances (scaleable)
â”œâ”€â”€ networks: federated-network
â”œâ”€â”€ volumes: shared data/models
â””â”€â”€ environment: API_BASE_URL, CLIENT_ID, etc.
```

## 8. PRIVACY & SECURITY FLOW

### 8.1 Differential Privacy
```
Opacus Integration (planned):
â”œâ”€â”€ PrivacyEngine.make_private()
â”œâ”€â”€ Clip gradients (max_grad_norm=1.0)
â”œâ”€â”€ Add noise (noise_multiplier=1.1)
â”œâ”€â”€ Track privacy budget (Îµ-spending)
â””â”€â”€ Îµ = 1.0 initial budget per hospital
```

### 8.2 Data Privacy
```
Hospital Data Never Leaves:
â”œâ”€â”€ Raw images stay local
â”œâ”€â”€ Only model updates transmitted
â”œâ”€â”€ Parameters encrypted in transit
â”œâ”€â”€ No patient identifiers in synthetic data
â””â”€â”€ Compliant with HIPAA/GDPR principles
```

### 8.3 Communication Security
```
Flower Framework Security:
â”œâ”€â”€ TLS encryption for all communications
â”œâ”€â”€ Certificate-based authentication
â”œâ”€â”€ Secure aggregation protocols
â””â”€â”€ Parameter compression/quantization
```

## 9. MONITORING & VISUALIZATION FLOW

### 9.1 Real-Time Metrics
```
During Training:
â”œâ”€â”€ Coordinator evaluates global model
â”œâ”€â”€ Posts metrics to API
â”œâ”€â”€ API stores in time-series
â””â”€â”€ Dashboard polls for updates
```

### 9.2 Dashboard Components (Planned)
```
Streamlit App:
â”œâ”€â”€ Round progress indicator
â”œâ”€â”€ Accuracy/loss charts over time
â”œâ”€â”€ Client participation map
â”œâ”€â”€ Privacy budget visualization
â””â”€â”€ Real-time log streaming
```

### 9.3 Performance Tracking
```
Metrics Collected:
â”œâ”€â”€ Per-round: loss, accuracy, AUC
â”œâ”€â”€ Per-client: contribution weight, local metrics
â”œâ”€â”€ Global: convergence rate, final performance
â””â”€â”€ System: training time, communication overhead
```

## 10. DEVELOPMENT PHASES

### Phase 1: Local Training âœ…
- Sanity check with synthetic data
- Validate model architecture
- Establish baseline performance

### Phase 2: Federated Learning âœ…
- Multi-hospital simulation
- Privacy-preserving aggregation
- Performance improvement demonstration

### Phase 3: Real-Time Dashboard ðŸ”„
- API integration
- Live monitoring
- Interactive visualizations

### Phase 4: Docker Deployment ðŸ”„
- Containerization
- Scalable orchestration
- Production-ready deployment

### Phase 5: Advanced Features ðŸ“‹
- Differential privacy
- Real medical datasets
- Multi-modal learning
- Regulatory compliance

## 11. ERROR HANDLING & EDGE CASES

### Common Issues:
```
Port Conflicts: Kill processes on 8080/5001
Memory Issues: Reduce batch_size or num_samples
Convergence Problems: Check data distribution, learning rate
API Failures: Verify CORS, payload formats
Docker Issues: Check container logs, network connectivity
```

### Validation Checks:
```
Pre-training: Data loading, model initialization
During training: Gradient flow, loss convergence
Post-training: Model saving, metrics calculation
API: Endpoint responses, data persistence
```

## 12. EXTENSIBILITY

### Adding New Models:
```
1. Implement in models/cnn_model.py
2. Update create_model() function
3. Test with local training
4. Integrate with federated client
```

### Adding New Datasets:
```
1. Create new Dataset class
2. Update get_dataloaders()
3. Handle preprocessing/augmentation
4. Test data distribution
```

### Adding New Metrics:
```
1. Update evaluate() function
2. Add to API payload structure
3. Update dashboard visualizations
4. Handle backward compatibility
```

This comprehensive flow document covers all aspects of the Healthcare Federated Learning Platform, from data generation to deployment, ensuring complete understanding of the system's operation and extensibility.
